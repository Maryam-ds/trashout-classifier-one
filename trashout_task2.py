

#!/usr/bin/env python
# coding: utf-8
#-------all required packages
import streamlit as st
import numpy as np
import pandas as pd
from sys import getsizeof
#from streamlit_folium import folium_static
#import folium
import requests
import matplotlib.pyplot as plt
import matplotlib.colors as colors
#from streamlit_folium import folium_static
import PIL
from PIL import Image
import requests
#------------------------------------
import tensorflow as tf
from tensorflow import one_hot
from tensorflow.keras.callbacks import LambdaCallback, Callback, ModelCheckpoint, EarlyStopping, LearningRateScheduler
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.models import load_model
from tensorflow.keras import Model, Sequential
from tensorflow.keras import backend, regularizers
from tensorflow.keras.layers import Activation, Reshape, Dense, Embedding, Dropout, Input, BatchNormalization, concatenate, Flatten, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adadelta, Adagrad, Adamax
#from tensorflow_addons.optimizers import AdamW
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.utils import to_categorical, plot_model
from shapely.geometry import Point
import time
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, confusion_matrix



# import seaborn as sns
# from sklearn.preprocessing import StandardScaler, MinMaxScaler
# from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, confusion_matrix



# from venues_deployed import getnearbyvenues
# from getnearbyvenues import *
# from mrcnn.infer import get_model,predict

from PIL import Image, ImageDraw, ImageFont
import os

st.set_option('deprecation.showfileUploaderEncoding', False)
pd.set_option('display.max_colwidth', None)

@st.cache(persist=True, allow_output_mutation=True, suppress_st_warning=True)
def process(image, server_url: str):
    
    m = MultipartEncoder(fields={'image':('filename', image, 'image/jpeg')})
    r = requests.post(server_url,data=m,headers={'Content-Type': m.content_type},timeout=8000)
    return r
@st.cache(persist=True, allow_output_mutation=True, suppress_st_warning=True)   

def file_selector(folder_path='.'):
    filenames = os.listdir(folder_path)
    selected_filename = st.selectbox('Select a file', filenames)
    return os.path.join(folder_path, selected_filename)

st.write('Trashout Shopping Product Classifier!')
INCEP_LABEL_PATH = "incep_cl/trained_labels.txt"
INCEP_MODEL_PATH = "incep_cl/trained_graph_4000.pb"

# MASKRCNN_MODEL_WEIGHTS = "mrcnn/weights/mask_rcnn_trashout_0250.h5"


option_model_first = st.sidebar.radio("Choose Option",options = ['Run product classifier','Regulations information'])
# option_model = st.sidebar.radio("Choose Option",options = ['Run product classifier','Regulations information'])



# Loads label file, strips off carriage return
label_lines = [line.rstrip() for line in tf.io.gfile.GFile(INCEP_LABEL_PATH)]
#Unpersists graph from file
with tf.compat.v1.gfile.FastGFile(INCEP_MODEL_PATH, "rb") as f:
    graph_def = tf.compat.v1.GraphDef()
    graph_def.ParseFromString(f.read())
    _ = tf.import_graph_def(graph_def, name="")

if option_model_first == 'Run product classifier':  
    load_image = uploaded_file = st.file_uploader("Load image", type=['jpg','png','jpeg'])
    if load_image is not None:
        image = Image.open(load_image)
        st.image(image, caption='Uploaded Image.', width = 160) 
        st.set_option('deprecation.showfileUploaderEncoding', False)
        option_model = st.selectbox("Choose Model" , options = ['Select Model','Inception','MaskRcnn'])
#         option_model = st.selectbox("Choose Model" , options = ['Select Model','Inception'])
   
        if (option_model == "Inception"):
            if st.button('Get Classification'):
                image = Image.open(load_image)
                load_image = image.convert('RGB')
                load_image.save('im.jpg', format="JPEG")
    
                image_data = tf.compat.v1.gfile.FastGFile('im.jpg', "rb").read()
                with tf.compat.v1.Session() as sess:             
                    # Feed the image_data as input to the graph and get first prediction
                    st.spinner=("Inference running on the Image by model...")
                    softmax_tensor = sess.graph.get_tensor_by_name("final_result:0")

                    predictions = sess.run(softmax_tensor, {"DecodeJpeg/contents:0": image_data})

                    # Sort to show labels of first prediction in order of confidence
                    top_k = predictions[0].argsort()[-len(predictions[0]) :][::-1]
                    answer = label_lines[top_k[0]]
                    print(answer)    
                image_label = answer
                #image_label = 'Felxible Plastic'
                st.image([image], image_label, width=160)  # output dyptich

#         elif (option_model == "MaskRcnn"):
#             if st.button('Get Classification'):
#                 image = Image.open(load_image)
#                 load_image = image.convert('RGB')
#                 load_image.save('im.jpg', format="JPEG")
#                 model =get_model(MASKRCNN_MODEL_WEIGHTS,'cpu')
#                 cats=predict('im.jpg',model)
#                 answer = cats[0]
#                 print(answer)
#                 image_label = answer
#                 #image_label = 'Felxible Plastic'
#                 st.image([image], image_label, width=160)  # output dyptich


# req_cols= ['City', 'Item Description', 'Disposal Category', 'Disposal Instructions']
# updated_df = pd.DataFrame(columns = req_cols)
# if option_model == 'Regulations information':
    
#     final_df = pd.DataFrame(columns =req_cols)
    
#     cg_data = pd.read_csv("communityGuidelines.csv",   engine='python')
#     df = cg_data[req_cols]
#     cities = df.drop_duplicates(subset='City')
#     cities_list = cities['City'].tolist()
#     city= st.selectbox("Select city", cities_list)
#     image_label_df= df.drop_duplicates(subset='Disposal Category')
#     list_imagelabel = image_label_df['Disposal Category'].tolist()
#     select_dc = st.selectbox("Select object" , options = list_imagelabel)
#     inds = df[(df["City"]==city) &  (select_dc == df["Disposal Category"])]
#     if(inds.empty):
#         st.write(city, 'guideline for disposing ',select_dc, 'is not available')
#     else:
#         disposal_instr = inds.iloc[0]['Disposal Instructions']
#         if(len(disposal_instr)==0 ):
#             st.write(city, 'guideline for disposing ',select_dc, 'is not available')
#         else:
#             st.write(city, 'guideline for disposing ',select_dc,': ', str(disposal_instr))
    
#     st.text("Can't find the disposal guidelines here and want to add it to our database ?")
#     st.text("Fill up the slots below and we shall add it in")
#     city_input = st.text_input("Enter the city for which you found the guideline")
    
#     final_df['City'] = pd.Series(city_input)
#     Item_Description = st.text_input("Provide an item description - example(glass bottle,plastic can,etc")
    
#     final_df['Item Description'] = Item_Description
#     Disposal_Category = st.text_input("Provide the disposal category - example(organicwaste,plastic,textile,glass,etc)")
    
#     final_df['Disposal Category']= Disposal_Category
#     Disposal_Instructions = st.text_input("Provide the disposal instructions")
    
#     final_df['Disposal Instructions']= Disposal_Instructions
    
#     add_data = st.button("Add info")
#     if add_data :
#         final_df
#         df
        
#         updated_df = updated_df.append(df)
#         updated_df = updated_df.append(final_df)
#         updated_df
#         updated_df.to_csv("communityGuidelines.csv",index =False)
#         final_df= final_df[0:0]      
        
    
    






